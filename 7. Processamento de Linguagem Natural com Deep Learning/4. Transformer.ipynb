{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47ac689b",
   "metadata": {},
   "source": [
    "# Transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ad3ab5",
   "metadata": {},
   "source": [
    "Referência: [Attention is all you need](https://arxiv.org/pdf/1706.03762.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e1fbb65",
   "metadata": {},
   "source": [
    "# Teoria\n",
    "------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbbb26e",
   "metadata": {},
   "source": [
    "## Introdução\n",
    "\n",
    "### RNNs clássicas para NLP\n",
    "\n",
    "- Palavras são codificadas em vetores\n",
    "- Cada novo estado é baseado no estado anterior\n",
    "- Decodificação começa no estado final do codificador\n",
    "![Rnns_classica](images/rnn_classica.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a292cc5a",
   "metadata": {},
   "source": [
    "### Mecanismo de atenção\n",
    "\n",
    "- ultima camada do codificador sobrecarregada com todos os textos\n",
    "- atenção maior para as camadas\n",
    "- maiores pesos para o contexto estado anterior\n",
    "\n",
    "![Mecanismo de atenção](images/attention.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e762d943",
   "metadata": {},
   "source": [
    "## Arquitetura \n",
    "\n",
    "Essa arquitetura tem como foco os mecanismos de atenção. Ao em vez de cada palavra receber a atenção, nos Transformers cada frase terá esse processo.\n",
    "\n",
    "![arquitetura](images/transformer.png)\n",
    "![arquitetura](images/self-attention.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f8c9ee",
   "metadata": {},
   "source": [
    "## Scale-dot product\n",
    "\n",
    "**Ideia principal:**\n",
    "- 2 sequências (iguais no caso de self-attention), A e B\n",
    "- calcular como cada elemento de A está relacionado a cada elmento de B\n",
    "- depois recombinamos A de acordo com essa relação\n",
    "\n",
    "**Matematicamente**, dot-product indica a similaridade entre dois vetores.\n",
    "\n",
    "![scale_dot-product](images/scale_dot-product.png)\n",
    "\n",
    "![scale_dot-product](images/scale_dot-product2.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab7eaaed",
   "metadata": {},
   "source": [
    "## Look-ahead Mask\n",
    "\n",
    "![look-ahead](images/look-ahead.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f5819d",
   "metadata": {},
   "source": [
    "## Attention Layer\n",
    "\n",
    "![attentio layer](images/attention-layer.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b95aed6",
   "metadata": {},
   "source": [
    "## Multi-head attention Layer\n",
    "\n",
    "![multi-head](images/multi-head.png)\n",
    "\n",
    "## Positional Encoding\n",
    "\n",
    "![positional-encod](images/positional-encod.png)\n",
    "\n",
    "## Feed-forward layers (camadas densas)\n",
    "\n",
    "- composta de 2 transformações lineares\n",
    "\n",
    "$$\n",
    "FFN(x) = \\max(0, x W_1 + b_1)W_2 + b_2\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd4b39f",
   "metadata": {},
   "source": [
    "## Residual connections:\n",
    "\n",
    "- **Add & Norm:** não esquecer a informação da etapa anterior, ajudando a aprendizagem durante o *backpropagation*.\n",
    "- **Last linear:** a saída do decodificador passa por uma camada densa de acordo com o tamanho do vocabulário e com a aplicação da função softmax, gerando probabilidades para cada palavra."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff205013",
   "metadata": {},
   "source": [
    "# Prática\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0668911a",
   "metadata": {},
   "source": [
    "## Importação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "d8f3bff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import math\n",
    "import re\n",
    "import time\n",
    "import zipfile\n",
    "import random\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_datasets as tfds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0621629e",
   "metadata": {},
   "source": [
    "# Preparação dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8841997a",
   "metadata": {},
   "source": [
    "- Bases de dados: https://www.statmt.org/europarl/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "75112094",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [283]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m../../_IAExpert_private/7. Processamento de Linguagem Natural com Deep LEarning/pt-en\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/europarl-v7.pt-en.en\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[1;32m----> 4\u001b[0m     europarl_en \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/europarl-v7.pt-en.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      7\u001b[0m     europarl_pt \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mread()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "file_path = '../../_IAExpert_private/7. Processamento de Linguagem Natural com Deep LEarning/pt-en'\n",
    "\n",
    "with open(f\"{file_path}/europarl-v7.pt-en.en\", mode='r', encoding='utf-8') as f:\n",
    "    europarl_en = f.read()\n",
    "\n",
    "with open(f\"{file_path}/europarl-v7.pt-en.pt\", mode='r', encoding='utf-8') as f:\n",
    "    europarl_pt = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bed9681",
   "metadata": {},
   "outputs": [],
   "source": [
    "europarl_en[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0ad9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "en = europarl_en.split('\\n')\n",
    "pt = europarl_pt.split('\\n')\n",
    "\n",
    "len(en), len(pt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da63152f",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(en)-1)\n",
    "en[i], pt[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4520058",
   "metadata": {},
   "source": [
    "### Limpeza dos dados"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f61b2b3a",
   "metadata": {},
   "source": [
    "```python\n",
    "corpus_en = europarl_en\n",
    "corpus_en = re.sub(r\"\\.(?=[0-9][a-z][A-Z])\", '.$$$', corpus_en)\n",
    "corpus_en = re.sub(r\".\\$\\$\\$\", '', corpus_en)\n",
    "corpus_en = re.sub(r\" +\", ' ', corpus_en)\n",
    "corpus_en = corpus_en.split('\\n')\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38e0543",
   "metadata": {},
   "source": [
    "```python\n",
    "corpus_pt = europarl_pt\n",
    "corpus_pt = re.sub(r\"\\.(?=[0-9][a-z][A-Z])\", '.$$$', corpus_pt)\n",
    "corpus_pt = re.sub(r\".\\$\\$\\$\", '', corpus_pt)\n",
    "corpus_pt = re.sub(r\" +\", ' ', corpus_pt)\n",
    "corpus_pt = corpus_pt.split('\\n')\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52278de7",
   "metadata": {},
   "source": [
    "```python\n",
    "with open(\"corpus.pkl\", \"wb\") as f:\n",
    "    pickle.dump([corpus_en, corpus_pt], f)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaeaf75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open(\"corpus.pkl\", 'rb') as f:\n",
    "    corpus_en, corpus_pt = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7f706f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(corpus_en), len(corpus_pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbb9b616",
   "metadata": {},
   "source": [
    "### Tokenização\n",
    "\n",
    "- texto para número"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0922c47c",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_en, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af3ed0cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_pt = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus(corpus_pt, target_vocab_size=2**13)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "979239be",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_en.vocab_size, tokenizer_pt.vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a958e38b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size_en = tokenizer_en.vocab_size + 2\n",
    "vocab_size_pt = tokenizer_pt.vocab_size + 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5da964",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [[vocab_size_en - 2] + tokenizer_en.encode(sentence) + [vocab_size_en - 1] for sentence in corpus_en]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1634c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = [[vocab_size_pt - 2] + tokenizer_pt.encode(sentence) + [vocab_size_pt - 1] for sentence in corpus_pt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29a87c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81b10ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91819139",
   "metadata": {},
   "source": [
    "### Remoção de sentenças muito longas"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7db336b0",
   "metadata": {},
   "source": [
    "#### A partir dos inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00b7f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15\n",
    "idx_to_remove = [count for count, sent in enumerate(inputs) if len(sent) > max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33a53243",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6893c6be",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6fd938",
   "metadata": {},
   "source": [
    "#### A partir dos outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50bdf59e",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 15\n",
    "idx_to_remove = [count for count, sent in enumerate(outputs) if len(sent) > max_length]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03350f9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(idx_to_remove)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f2fb766",
   "metadata": {},
   "outputs": [],
   "source": [
    "for idx in reversed(idx_to_remove):\n",
    "    del inputs[idx]\n",
    "    del outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f137e1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(inputs), len(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db81acd",
   "metadata": {},
   "source": [
    "### Paddings e batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef957d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs, value=0, padding='post', maxlen=max_length)\n",
    "outputs = tf.keras.preprocessing.sequence.pad_sequences(outputs, value=0, padding='post', maxlen=max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c89f12d",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = random.randint(0, len(inputs))\n",
    "print(i, len(inputs[i]), len(outputs[i]))\n",
    "inputs[i], outputs[i]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac660d4",
   "metadata": {},
   "source": [
    "Vamos mudar para o formato do tensor flow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d4b3bf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "buffer_size = 20000\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices((inputs, outputs))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(buffer_size).batch(batch_size)\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8480b68",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5ace9f",
   "metadata": {},
   "source": [
    "# Modelo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c74a9da",
   "metadata": {},
   "source": [
    "## Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "id": "78df1b50",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(layers.Layer):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angles = 1 / np.power(10000., (2*(i // 2)) / np.float32(d_model))\n",
    "        return pos * angles\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        seq_length = inputs.shape.as_list()[-2]\n",
    "        d_model = inputs.shape.as_list()[-1]\n",
    "        angles = self.get_angles(np.arange(seq_length)[:, np.newaxis], \n",
    "                                 np.arange(d_model)[np.newaxis,:], \n",
    "                                 d_model)\n",
    "        \n",
    "        angles[:, 0::2] = np.sin(angles[:, 0::2])\n",
    "        angles[:, 1::2] = np.cos(angles[:, 1::2])\n",
    "        pos_encoding = angles[np.newaxis, ...]\n",
    "        \n",
    "        return inputs + tf.cast(pos_encoding, tf.float32)        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b42acac8",
   "metadata": {},
   "source": [
    "## Mecanismo de atenção"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "3fd4cf13",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(queries, keys, values, mask):\n",
    "    product = tf.matmul(queries, keys, transpose_b=True)\n",
    "    keys_dim = tf.cast(tf.shape(keys)[-1], tf.float32)\n",
    "    scaled_product = product / tf.math.sqrt(keys_dim)\n",
    "\n",
    "    if mask is not None:\n",
    "        scaled_product += (mask * -1e9) # 0.0000000001\n",
    "\n",
    "    attention = tf.matmul(tf.nn.softmax(scaled_product, axis=-1), values)\n",
    "    return attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "01fc7bd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(layers.Layer):\n",
    "    \n",
    "    def __init__(self, nb_proj):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.nb_proj = nb_proj\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        assert self.d_model % self.nb_proj == 0\n",
    "\n",
    "        self.d_proj = self.d_model // self.nb_proj\n",
    "\n",
    "        self.query_lin = layers.Dense(units = self.d_model)\n",
    "        self.key_lin = layers.Dense(units = self.d_model)\n",
    "        self.value_lin = layers.Dense(units = self.d_model)\n",
    "\n",
    "        self.final_lin = layers.Dense(units = self.d_model)\n",
    "    \n",
    "    def split_proj(self, inputs, batch_size): \n",
    "        shape = (batch_size, -1, self.nb_proj, self.d_proj)\n",
    "        splited_inputs = tf.reshape(inputs, shape = shape) \n",
    "        return tf.transpose(splited_inputs, perm=[0, 2, 1, 3]) \n",
    "   \n",
    "    def call(self, queries, keys, values, mask):\n",
    "        batch_size = tf.shape(queries)[0]\n",
    "\n",
    "        queries = self.query_lin(queries)\n",
    "        keys = self.key_lin(keys)\n",
    "        values = self.value_lin(values)\n",
    "\n",
    "        queries = self.split_proj(queries, batch_size)\n",
    "        keys = self.split_proj(keys, batch_size)\n",
    "        values = self.split_proj(values, batch_size)\n",
    "\n",
    "        attention = scaled_dot_product_attention(queries, keys, values, mask)\n",
    "\n",
    "        attention = tf.transpose(attention, perm=[0, 2, 1, 3])\n",
    "\n",
    "        concat_attention = tf.reshape(attention, shape=(batch_size, -1, self.d_model))\n",
    "\n",
    "        outputs = self.final_lin(concat_attention)\n",
    "      \n",
    "        return outputs    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98b350f",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "5448a463",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        \n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, mask, training):\n",
    "        attention = self.multi_head_attention(inputs, inputs, inputs, mask)        \n",
    "        attention = self.dropout_1(attention, training=training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        outputs = self.dense_1(attention)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_2(outputs, training=training)\n",
    "        outputs = self.norm_2(outputs + attention)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "6c103c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "               nb_layers, \n",
    "               FFN_units, \n",
    "               nb_proj, \n",
    "               dropout_rate, \n",
    "               vocab_size, \n",
    "               d_model, \n",
    "               name='encoder'):\n",
    "        super(Encoder, self).__init__(name=name)\n",
    "        self.nb_layers = nb_layers\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        self.enc_layers = [EncoderLayer(FFN_units, nb_proj, dropout_rate) for _ in range(nb_layers)]\n",
    "    \n",
    "    def call(self, inputs, mask, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.enc_layers[i](outputs, mask, training)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0edf3e3d",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "abb2bfd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(layers.Layer):\n",
    "    \n",
    "    def __init__(self, FFN_units, nb_proj, dropout_rate):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.FFN_units = FFN_units\n",
    "        self.nb_proj = nb_proj\n",
    "        self.dropout_rate = dropout_rate\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.d_model = input_shape[-1]\n",
    "        \n",
    "        self.multi_head_attention_1 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_1 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_1 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.multi_head_attention_2 = MultiHeadAttention(self.nb_proj)\n",
    "        self.dropout_2 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_2 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "        self.dense_1 = layers.Dense(units=self.FFN_units, activation='relu')\n",
    "        self.dense_2 = layers.Dense(units=self.d_model, activation='relu')\n",
    "        self.dropout_3 = layers.Dropout(rate=self.dropout_rate)\n",
    "        self.norm_3 = layers.LayerNormalization(epsilon=1e-6)\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        attention = self.multi_head_attention_1(inputs, inputs, inputs, mask_1)\n",
    "        attention = self.dropout_1(attention, training)\n",
    "        attention = self.norm_1(attention + inputs)\n",
    "        \n",
    "        attention_2 = self.multi_head_attention_2(attention, enc_outputs, enc_outputs, mask_2)\n",
    "        attention_2 = self.dropout_2(attention_2, training)\n",
    "        attention_2 = self.norm_2(attention_2 + attention)\n",
    "        \n",
    "        outputs = self.dense_1(attention_2)\n",
    "        outputs = self.dense_2(outputs)\n",
    "        outputs = self.dropout_3(outputs, training)\n",
    "        outputs = self.norm_3(outputs + attention_2)\n",
    "        \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "2114fe6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(layers.Layer):\n",
    "    \n",
    "    def __init__(self, \n",
    "                 nb_layers, \n",
    "                 FFN_units, \n",
    "                 nb_proj, \n",
    "                 dropout_rate, \n",
    "                 vocab_size, \n",
    "                 d_model, \n",
    "                 name='encoder'):\n",
    "        super(Decoder, self).__init__(name=name)\n",
    "        self.d_model = d_model\n",
    "        self.nb_layers = nb_layers\n",
    "        \n",
    "        self.embedding = layers.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoding = PositionalEncoding()\n",
    "        self.dropout = layers.Dropout(rate=dropout_rate)\n",
    "        \n",
    "        self.dec_layers = [DecoderLayer(FFN_units, nb_proj, dropout_rate) for i in range(nb_layers)]\n",
    "        \n",
    "    def call(self, inputs, enc_outputs, mask_1, mask_2, training):\n",
    "        outputs = self.embedding(inputs)\n",
    "        outputs *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        outputs = self.pos_encoding(outputs)\n",
    "        outputs = self.dropout(outputs, training)\n",
    "        \n",
    "        for i in range(self.nb_layers):\n",
    "            outputs = self.dec_layers[i](outputs, enc_outputs, mask_1, mask_2, training)\n",
    "            \n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a4d2a76",
   "metadata": {},
   "source": [
    "## Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "9c9ce76c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    \n",
    "    def __init__(self,\n",
    "                   vocab_size_enc,\n",
    "                   vocab_size_dec,\n",
    "                   d_model,\n",
    "                 nb_layers,\n",
    "                   FFN_units,\n",
    "                   nb_proj,\n",
    "                   dropout_rate,\n",
    "                   name=\"transformer\"):\n",
    "        super(Transformer, self).__init__(name=name)\n",
    "\n",
    "        self.encoder = Encoder(nb_layers, FFN_units, nb_proj, dropout_rate, \n",
    "                               vocab_size_enc, d_model)\n",
    "        self.decoder = Decoder(nb_layers, FFN_units, nb_proj, dropout_rate,\n",
    "                               vocab_size_dec, d_model)\n",
    "        self.last_linear = layers.Dense(units=vocab_size_dec, name='lin_output')\n",
    "                \n",
    "    def create_padding_mask(self, seq): \n",
    "        mask = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "        \n",
    "        return mask[:, tf.newaxis, tf.newaxis, :]\n",
    "    \n",
    "    def create_look_ahead_mask(self, seq):\n",
    "        seq_len = tf.shape(seq)[1]\n",
    "        look_ahed_mask = 1 - tf.linalg.band_part(tf.ones((seq_len, seq_len)), -1, 0)\n",
    "        return look_ahed_mask\n",
    "    \n",
    "    def call(self, enc_inputs, dec_inputs, training):\n",
    "        enc_mask = self.create_padding_mask(enc_inputs)\n",
    "        dec_mask_1 = tf.maximum(self.create_padding_mask(dec_inputs), self.create_look_ahead_mask(dec_inputs))\n",
    "        dec_mask_2 = self.create_padding_mask(enc_inputs)\n",
    "\n",
    "        enc_outputs = self.encoder(enc_inputs, enc_mask, training)\n",
    "        dec_outputs = self.decoder(dec_inputs, enc_outputs, dec_mask_1, dec_mask_2, training)\n",
    "\n",
    "        outputs = self.last_linear(dec_outputs)\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5241b0d3",
   "metadata": {},
   "source": [
    "# Treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "b3607531",
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.keras.backend.clear_session()\n",
    "\n",
    "d_model = 128\n",
    "nb_layers = 4\n",
    "ffn_units = 512\n",
    "nb_proj = 8\n",
    "dropout_rate = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "b558af23",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(vocab_size_enc=vocab_size_en, \n",
    "                          vocab_size_dec=vocab_size_pt, \n",
    "                          d_model=d_model, \n",
    "                          nb_layers=nb_layers,\n",
    "                          FFN_units=ffn_units, \n",
    "                          nb_proj=nb_proj, \n",
    "                          dropout_rate=dropout_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "268ee1af",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "86cb70e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def loss_function(target, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(target, 0))\n",
    "    loss_ = loss_object(target, pred)\n",
    "\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_mean(loss_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "51e5f6e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_accuracy = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "ef10ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    \n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        \n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "        \n",
    "    def __call__(self, step):\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "081d3f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = CustomSchedule(d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "35db8931",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate, beta_1=0.9, beta_2=0.98, epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "ffc95911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start or epoch 1\n",
      "Epoch 1 Batch 0 Loss 6.2424 Accuracy 0.0000\n",
      "Epoch 1 Batch 50 Loss 6.3664 Accuracy 0.0204\n",
      "Epoch 1 Batch 100 Loss 6.2987 Accuracy 0.0457\n",
      "Epoch 1 Batch 150 Loss 6.2138 Accuracy 0.0542\n",
      "Epoch 1 Batch 200 Loss 6.1287 Accuracy 0.0585\n",
      "Epoch 1 Batch 250 Loss 6.0355 Accuracy 0.0611\n",
      "Epoch 1 Batch 300 Loss 5.9247 Accuracy 0.0628\n",
      "Epoch 1 Batch 350 Loss 5.7906 Accuracy 0.0652\n",
      "Epoch 1 Batch 400 Loss 5.6493 Accuracy 0.0709\n",
      "Epoch 1 Batch 450 Loss 5.5287 Accuracy 0.0769\n",
      "Epoch 1 Batch 500 Loss 5.4141 Accuracy 0.0820\n",
      "Epoch 1 Batch 550 Loss 5.3062 Accuracy 0.0868\n",
      "Epoch 1 Batch 600 Loss 5.2063 Accuracy 0.0920\n",
      "Epoch 1 Batch 650 Loss 5.1108 Accuracy 0.0972\n",
      "Epoch 1 Batch 700 Loss 5.0182 Accuracy 0.1023\n",
      "Epoch 1 Batch 750 Loss 4.9381 Accuracy 0.1076\n",
      "Epoch 1 Batch 800 Loss 4.8591 Accuracy 0.1127\n",
      "Epoch 1 Batch 850 Loss 4.7847 Accuracy 0.1175\n",
      "Epoch 1 Batch 900 Loss 4.7125 Accuracy 0.1223\n",
      "Epoch 1 Batch 950 Loss 4.6452 Accuracy 0.1269\n",
      "Epoch 1 Batch 1000 Loss 4.5791 Accuracy 0.1312\n",
      "Epoch 1 Batch 1050 Loss 4.5156 Accuracy 0.1351\n",
      "Epoch 1 Batch 1100 Loss 4.4543 Accuracy 0.1389\n",
      "Epoch 1 Batch 1150 Loss 4.3928 Accuracy 0.1425\n",
      "Epoch 1 Batch 1200 Loss 4.3353 Accuracy 0.1462\n",
      "Epoch 1 Batch 1250 Loss 4.2804 Accuracy 0.1499\n",
      "Epoch 1 Batch 1300 Loss 4.2270 Accuracy 0.1536\n",
      "Epoch 1 Batch 1350 Loss 4.1782 Accuracy 0.1574\n",
      "Epoch 1 Batch 1400 Loss 4.1279 Accuracy 0.1612\n",
      "Epoch 1 Batch 1450 Loss 4.0816 Accuracy 0.1649\n",
      "Epoch 1 Batch 1500 Loss 4.0332 Accuracy 0.1686\n",
      "Epoch 1 Batch 1550 Loss 3.9898 Accuracy 0.1721\n",
      "Epoch 1 Batch 1600 Loss 3.9499 Accuracy 0.1754\n",
      "Epoch 1 Batch 1650 Loss 3.9105 Accuracy 0.1787\n",
      "Epoch 1 Batch 1700 Loss 3.8699 Accuracy 0.1821\n",
      "Epoch 1 Batch 1750 Loss 3.8329 Accuracy 0.1854\n",
      "Epoch 1 Batch 1800 Loss 3.7970 Accuracy 0.1886\n",
      "Epoch 1 Batch 1850 Loss 3.7633 Accuracy 0.1916\n",
      "Epoch 1 Batch 1900 Loss 3.7300 Accuracy 0.1945\n",
      "Epoch 1 Batch 1950 Loss 3.6981 Accuracy 0.1974\n",
      "Epoch 1 Batch 2000 Loss 3.6654 Accuracy 0.2004\n",
      "Epoch 1 Batch 2050 Loss 3.6350 Accuracy 0.2033\n",
      "Epoch 1 Batch 2100 Loss 3.6068 Accuracy 0.2060\n",
      "Epoch 1 Batch 2150 Loss 3.5784 Accuracy 0.2086\n",
      "Epoch 1 Batch 2200 Loss 3.5515 Accuracy 0.2111\n",
      "Epoch 1 Batch 2250 Loss 3.5249 Accuracy 0.2136\n",
      "Epoch 1 Batch 2300 Loss 3.4984 Accuracy 0.2159\n",
      "Epoch 1 Batch 2350 Loss 3.4759 Accuracy 0.2180\n",
      "Epoch 1 Batch 2400 Loss 3.4539 Accuracy 0.2201\n",
      "Epoch 1 Batch 2450 Loss 3.4318 Accuracy 0.2220\n",
      "Epoch 1 Batch 2500 Loss 3.4110 Accuracy 0.2240\n",
      "Epoch 1 Batch 2550 Loss 3.3915 Accuracy 0.2260\n",
      "Epoch 1 Batch 2600 Loss 3.3724 Accuracy 0.2278\n",
      "Epoch 1 Batch 2650 Loss 3.3530 Accuracy 0.2296\n",
      "Epoch 1 Batch 2700 Loss 3.3341 Accuracy 0.2313\n",
      "Epoch 1 Batch 2750 Loss 3.3167 Accuracy 0.2331\n",
      "Epoch 1 Batch 2800 Loss 3.2993 Accuracy 0.2347\n",
      "Epoch 1 Batch 2850 Loss 3.2821 Accuracy 0.2364\n",
      "Epoch 1 Batch 2900 Loss 3.2646 Accuracy 0.2380\n",
      "Epoch 1 Batch 2950 Loss 3.2475 Accuracy 0.2395\n",
      "Epoch 1 Batch 3000 Loss 3.2307 Accuracy 0.2410\n",
      "Epoch 1 Batch 3050 Loss 3.2141 Accuracy 0.2426\n",
      "Epoch 1 Batch 3100 Loss 3.1981 Accuracy 0.2441\n",
      "Epoch 1 Batch 3150 Loss 3.1816 Accuracy 0.2456\n",
      "Epoch 1 Batch 3200 Loss 3.1666 Accuracy 0.2471\n",
      "Epoch 1 Batch 3250 Loss 3.1515 Accuracy 0.2486\n",
      "Time taken for 1 epoch 3524.847275018692 secs\n",
      "\n",
      "Start or epoch 2\n",
      "Epoch 2 Batch 0 Loss 2.5222 Accuracy 0.3449\n",
      "Epoch 2 Batch 50 Loss 2.2546 Accuracy 0.3386\n",
      "Epoch 2 Batch 100 Loss 2.2203 Accuracy 0.3434\n",
      "Epoch 2 Batch 150 Loss 2.1986 Accuracy 0.3448\n",
      "Epoch 2 Batch 200 Loss 2.1845 Accuracy 0.3455\n",
      "Epoch 2 Batch 250 Loss 2.1720 Accuracy 0.3468\n",
      "Epoch 2 Batch 300 Loss 2.1654 Accuracy 0.3475\n",
      "Epoch 2 Batch 350 Loss 2.1520 Accuracy 0.3489\n",
      "Epoch 2 Batch 400 Loss 2.1415 Accuracy 0.3490\n",
      "Epoch 2 Batch 450 Loss 2.1307 Accuracy 0.3493\n",
      "Epoch 2 Batch 500 Loss 2.1206 Accuracy 0.3496\n",
      "Epoch 2 Batch 550 Loss 2.1082 Accuracy 0.3502\n",
      "Epoch 2 Batch 600 Loss 2.1041 Accuracy 0.3504\n",
      "Epoch 2 Batch 650 Loss 2.0946 Accuracy 0.3510\n",
      "Epoch 2 Batch 700 Loss 2.0854 Accuracy 0.3516\n",
      "Epoch 2 Batch 750 Loss 2.0781 Accuracy 0.3526\n",
      "Epoch 2 Batch 800 Loss 2.0699 Accuracy 0.3531\n",
      "Epoch 2 Batch 850 Loss 2.0628 Accuracy 0.3536\n",
      "Epoch 2 Batch 900 Loss 2.0544 Accuracy 0.3541\n",
      "Epoch 2 Batch 950 Loss 2.0444 Accuracy 0.3548\n",
      "Epoch 2 Batch 1000 Loss 2.0343 Accuracy 0.3557\n",
      "Epoch 2 Batch 1050 Loss 2.0227 Accuracy 0.3568\n",
      "Epoch 2 Batch 1100 Loss 2.0104 Accuracy 0.3577\n",
      "Epoch 2 Batch 1150 Loss 1.9967 Accuracy 0.3590\n",
      "Epoch 2 Batch 1200 Loss 1.9841 Accuracy 0.3600\n",
      "Epoch 2 Batch 1250 Loss 1.9707 Accuracy 0.3614\n",
      "Epoch 2 Batch 1300 Loss 1.9580 Accuracy 0.3627\n",
      "Epoch 2 Batch 1350 Loss 1.9468 Accuracy 0.3640\n",
      "Epoch 2 Batch 1400 Loss 1.9361 Accuracy 0.3653\n",
      "Epoch 2 Batch 1450 Loss 1.9258 Accuracy 0.3666\n",
      "Epoch 2 Batch 1500 Loss 1.9158 Accuracy 0.3679\n",
      "Epoch 2 Batch 1550 Loss 1.9052 Accuracy 0.3694\n",
      "Epoch 2 Batch 1600 Loss 1.8960 Accuracy 0.3706\n",
      "Epoch 2 Batch 1650 Loss 1.8847 Accuracy 0.3720\n",
      "Epoch 2 Batch 1700 Loss 1.8753 Accuracy 0.3733\n",
      "Epoch 2 Batch 1750 Loss 1.8663 Accuracy 0.3743\n",
      "Epoch 2 Batch 1800 Loss 1.8589 Accuracy 0.3755\n",
      "Epoch 2 Batch 1850 Loss 1.8508 Accuracy 0.3766\n",
      "Epoch 2 Batch 1900 Loss 1.8434 Accuracy 0.3778\n",
      "Epoch 2 Batch 1950 Loss 1.8362 Accuracy 0.3792\n",
      "Epoch 2 Batch 2000 Loss 1.8280 Accuracy 0.3804\n",
      "Epoch 2 Batch 2050 Loss 1.8211 Accuracy 0.3817\n",
      "Epoch 2 Batch 2100 Loss 1.8152 Accuracy 0.3827\n",
      "Epoch 2 Batch 2150 Loss 1.8073 Accuracy 0.3839\n",
      "Epoch 2 Batch 2200 Loss 1.8002 Accuracy 0.3852\n",
      "Epoch 2 Batch 2250 Loss 1.7942 Accuracy 0.3863\n",
      "Epoch 2 Batch 2300 Loss 1.7896 Accuracy 0.3871\n",
      "Epoch 2 Batch 2350 Loss 1.7866 Accuracy 0.3877\n",
      "Epoch 2 Batch 2400 Loss 1.7838 Accuracy 0.3883\n",
      "Epoch 2 Batch 2450 Loss 1.7815 Accuracy 0.3888\n",
      "Epoch 2 Batch 2500 Loss 1.7794 Accuracy 0.3892\n",
      "Epoch 2 Batch 2550 Loss 1.7782 Accuracy 0.3896\n",
      "Epoch 2 Batch 2600 Loss 1.7762 Accuracy 0.3901\n",
      "Epoch 2 Batch 2650 Loss 1.7742 Accuracy 0.3905\n",
      "Epoch 2 Batch 2700 Loss 1.7725 Accuracy 0.3908\n",
      "Epoch 2 Batch 2750 Loss 1.7713 Accuracy 0.3910\n",
      "Epoch 2 Batch 2800 Loss 1.7702 Accuracy 0.3913\n",
      "Epoch 2 Batch 2850 Loss 1.7692 Accuracy 0.3916\n",
      "Epoch 2 Batch 2900 Loss 1.7673 Accuracy 0.3919\n",
      "Epoch 2 Batch 2950 Loss 1.7658 Accuracy 0.3922\n",
      "Epoch 2 Batch 3000 Loss 1.7644 Accuracy 0.3925\n",
      "Epoch 2 Batch 3050 Loss 1.7632 Accuracy 0.3927\n",
      "Epoch 2 Batch 3100 Loss 1.7612 Accuracy 0.3930\n",
      "Epoch 2 Batch 3150 Loss 1.7594 Accuracy 0.3932\n",
      "Epoch 2 Batch 3200 Loss 1.7573 Accuracy 0.3936\n",
      "Epoch 2 Batch 3250 Loss 1.7560 Accuracy 0.3939\n",
      "Time taken for 1 epoch 3290.0849902629852 secs\n",
      "\n",
      "Start or epoch 3\n",
      "Epoch 3 Batch 0 Loss 1.5701 Accuracy 0.3571\n",
      "Epoch 3 Batch 50 Loss 1.6979 Accuracy 0.4149\n",
      "Epoch 3 Batch 100 Loss 1.6836 Accuracy 0.4130\n",
      "Epoch 3 Batch 150 Loss 1.6597 Accuracy 0.4138\n",
      "Epoch 3 Batch 200 Loss 1.6551 Accuracy 0.4151\n",
      "Epoch 3 Batch 250 Loss 1.6480 Accuracy 0.4160\n",
      "Epoch 3 Batch 300 Loss 1.6425 Accuracy 0.4169\n",
      "Epoch 3 Batch 350 Loss 1.6281 Accuracy 0.4172\n",
      "Epoch 3 Batch 400 Loss 1.6170 Accuracy 0.4178\n",
      "Epoch 3 Batch 450 Loss 1.6083 Accuracy 0.4183\n",
      "Epoch 3 Batch 500 Loss 1.6020 Accuracy 0.4187\n",
      "Epoch 3 Batch 550 Loss 1.5930 Accuracy 0.4190\n",
      "Epoch 3 Batch 600 Loss 1.5870 Accuracy 0.4195\n",
      "Epoch 3 Batch 650 Loss 1.5771 Accuracy 0.4197\n",
      "Epoch 3 Batch 700 Loss 1.5689 Accuracy 0.4206\n",
      "Epoch 3 Batch 750 Loss 1.5631 Accuracy 0.4215\n",
      "Epoch 3 Batch 800 Loss 1.5578 Accuracy 0.4220\n",
      "Epoch 3 Batch 850 Loss 1.5511 Accuracy 0.4223\n",
      "Epoch 3 Batch 900 Loss 1.5439 Accuracy 0.4226\n",
      "Epoch 3 Batch 950 Loss 1.5385 Accuracy 0.4232\n",
      "Epoch 3 Batch 1000 Loss 1.5308 Accuracy 0.4236\n",
      "Epoch 3 Batch 1050 Loss 1.5239 Accuracy 0.4240\n",
      "Epoch 3 Batch 1100 Loss 1.5156 Accuracy 0.4245\n",
      "Epoch 3 Batch 1150 Loss 1.5064 Accuracy 0.4250\n",
      "Epoch 3 Batch 1200 Loss 1.4978 Accuracy 0.4257\n",
      "Epoch 3 Batch 1250 Loss 1.4880 Accuracy 0.4264\n",
      "Epoch 3 Batch 1300 Loss 1.4805 Accuracy 0.4272\n",
      "Epoch 3 Batch 1350 Loss 1.4728 Accuracy 0.4280\n",
      "Epoch 3 Batch 1400 Loss 1.4642 Accuracy 0.4288\n",
      "Epoch 3 Batch 1450 Loss 1.4582 Accuracy 0.4298\n",
      "Epoch 3 Batch 1500 Loss 1.4514 Accuracy 0.4305\n",
      "Epoch 3 Batch 1550 Loss 1.4444 Accuracy 0.4313\n",
      "Epoch 3 Batch 1600 Loss 1.4385 Accuracy 0.4320\n",
      "Epoch 3 Batch 1650 Loss 1.4319 Accuracy 0.4329\n",
      "Epoch 3 Batch 1700 Loss 1.4261 Accuracy 0.4338\n",
      "Epoch 3 Batch 1750 Loss 1.4205 Accuracy 0.4346\n",
      "Epoch 3 Batch 1800 Loss 1.4157 Accuracy 0.4354\n",
      "Epoch 3 Batch 1850 Loss 1.4108 Accuracy 0.4363\n",
      "Epoch 3 Batch 1900 Loss 1.4071 Accuracy 0.4372\n",
      "Epoch 3 Batch 1950 Loss 1.4023 Accuracy 0.4379\n",
      "Epoch 3 Batch 2000 Loss 1.3981 Accuracy 0.4387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3 Batch 2050 Loss 1.3948 Accuracy 0.4395\n",
      "Epoch 3 Batch 2100 Loss 1.3903 Accuracy 0.4404\n",
      "Epoch 3 Batch 2150 Loss 1.3867 Accuracy 0.4411\n",
      "Epoch 3 Batch 2200 Loss 1.3835 Accuracy 0.4420\n",
      "Epoch 3 Batch 2250 Loss 1.3813 Accuracy 0.4425\n",
      "Epoch 3 Batch 2300 Loss 1.3796 Accuracy 0.4429\n",
      "Epoch 3 Batch 2350 Loss 1.3790 Accuracy 0.4432\n",
      "Epoch 3 Batch 2400 Loss 1.3790 Accuracy 0.4434\n",
      "Epoch 3 Batch 2450 Loss 1.3793 Accuracy 0.4435\n",
      "Epoch 3 Batch 2500 Loss 1.3802 Accuracy 0.4435\n",
      "Epoch 3 Batch 2550 Loss 1.3819 Accuracy 0.4434\n",
      "Epoch 3 Batch 2600 Loss 1.3831 Accuracy 0.4434\n",
      "Epoch 3 Batch 2650 Loss 1.3846 Accuracy 0.4434\n",
      "Epoch 3 Batch 2700 Loss 1.3860 Accuracy 0.4434\n",
      "Epoch 3 Batch 2750 Loss 1.3870 Accuracy 0.4434\n",
      "Epoch 3 Batch 2800 Loss 1.3890 Accuracy 0.4433\n",
      "Epoch 3 Batch 2850 Loss 1.3900 Accuracy 0.4432\n",
      "Epoch 3 Batch 2900 Loss 1.3908 Accuracy 0.4431\n",
      "Epoch 3 Batch 2950 Loss 1.3918 Accuracy 0.4431\n",
      "Epoch 3 Batch 3000 Loss 1.3931 Accuracy 0.4430\n",
      "Epoch 3 Batch 3050 Loss 1.3940 Accuracy 0.4429\n",
      "Epoch 3 Batch 3100 Loss 1.3949 Accuracy 0.4428\n",
      "Epoch 3 Batch 3150 Loss 1.3956 Accuracy 0.4427\n",
      "Epoch 3 Batch 3200 Loss 1.3959 Accuracy 0.4427\n",
      "Epoch 3 Batch 3250 Loss 1.3965 Accuracy 0.4427\n",
      "Time taken for 1 epoch 3226.8729095458984 secs\n",
      "\n",
      "Start or epoch 4\n",
      "Epoch 4 Batch 0 Loss 1.7027 Accuracy 0.4252\n",
      "Epoch 4 Batch 50 Loss 1.4944 Accuracy 0.4390\n",
      "Epoch 4 Batch 100 Loss 1.4708 Accuracy 0.4395\n",
      "Epoch 4 Batch 150 Loss 1.4665 Accuracy 0.4406\n",
      "Epoch 4 Batch 200 Loss 1.4592 Accuracy 0.4407\n",
      "Epoch 4 Batch 250 Loss 1.4460 Accuracy 0.4427\n",
      "Epoch 4 Batch 300 Loss 1.4375 Accuracy 0.4439\n",
      "Epoch 4 Batch 350 Loss 1.4299 Accuracy 0.4440\n",
      "Epoch 4 Batch 400 Loss 1.4180 Accuracy 0.4441\n",
      "Epoch 4 Batch 450 Loss 1.4111 Accuracy 0.4442\n",
      "Epoch 4 Batch 500 Loss 1.4017 Accuracy 0.4447\n",
      "Epoch 4 Batch 550 Loss 1.3965 Accuracy 0.4451\n",
      "Epoch 4 Batch 600 Loss 1.3894 Accuracy 0.4455\n",
      "Epoch 4 Batch 650 Loss 1.3842 Accuracy 0.4457\n",
      "Epoch 4 Batch 700 Loss 1.3773 Accuracy 0.4459\n",
      "Epoch 4 Batch 750 Loss 1.3723 Accuracy 0.4464\n",
      "Epoch 4 Batch 800 Loss 1.3654 Accuracy 0.4472\n",
      "Epoch 4 Batch 850 Loss 1.3608 Accuracy 0.4475\n",
      "Epoch 4 Batch 900 Loss 1.3570 Accuracy 0.4478\n",
      "Epoch 4 Batch 950 Loss 1.3512 Accuracy 0.4480\n",
      "Epoch 4 Batch 1000 Loss 1.3453 Accuracy 0.4485\n",
      "Epoch 4 Batch 1050 Loss 1.3389 Accuracy 0.4487\n",
      "Epoch 4 Batch 1100 Loss 1.3296 Accuracy 0.4492\n",
      "Epoch 4 Batch 1150 Loss 1.3216 Accuracy 0.4497\n",
      "Epoch 4 Batch 1200 Loss 1.3138 Accuracy 0.4504\n",
      "Epoch 4 Batch 1250 Loss 1.3074 Accuracy 0.4509\n",
      "Epoch 4 Batch 1300 Loss 1.3012 Accuracy 0.4515\n",
      "Epoch 4 Batch 1350 Loss 1.2934 Accuracy 0.4523\n",
      "Epoch 4 Batch 1400 Loss 1.2881 Accuracy 0.4530\n",
      "Epoch 4 Batch 1450 Loss 1.2830 Accuracy 0.4537\n",
      "Epoch 4 Batch 1500 Loss 1.2774 Accuracy 0.4544\n",
      "Epoch 4 Batch 1550 Loss 1.2714 Accuracy 0.4552\n",
      "Epoch 4 Batch 1600 Loss 1.2665 Accuracy 0.4559\n",
      "Epoch 4 Batch 1650 Loss 1.2616 Accuracy 0.4566\n",
      "Epoch 4 Batch 1700 Loss 1.2571 Accuracy 0.4573\n",
      "Epoch 4 Batch 1750 Loss 1.2523 Accuracy 0.4581\n",
      "Epoch 4 Batch 1800 Loss 1.2479 Accuracy 0.4590\n",
      "Epoch 4 Batch 1850 Loss 1.2438 Accuracy 0.4597\n",
      "Epoch 4 Batch 1900 Loss 1.2409 Accuracy 0.4605\n",
      "Epoch 4 Batch 1950 Loss 1.2369 Accuracy 0.4613\n",
      "Epoch 4 Batch 2000 Loss 1.2336 Accuracy 0.4619\n",
      "Epoch 4 Batch 2050 Loss 1.2308 Accuracy 0.4628\n",
      "Epoch 4 Batch 2100 Loss 1.2272 Accuracy 0.4635\n",
      "Epoch 4 Batch 2150 Loss 1.2247 Accuracy 0.4643\n",
      "Epoch 4 Batch 2200 Loss 1.2224 Accuracy 0.4649\n",
      "Epoch 4 Batch 2250 Loss 1.2198 Accuracy 0.4652\n",
      "Epoch 4 Batch 2300 Loss 1.2188 Accuracy 0.4656\n",
      "Epoch 4 Batch 2350 Loss 1.2182 Accuracy 0.4658\n",
      "Epoch 4 Batch 2400 Loss 1.2186 Accuracy 0.4658\n",
      "Epoch 4 Batch 2450 Loss 1.2201 Accuracy 0.4658\n",
      "Epoch 4 Batch 2500 Loss 1.2216 Accuracy 0.4657\n",
      "Epoch 4 Batch 2550 Loss 1.2230 Accuracy 0.4656\n",
      "Epoch 4 Batch 2600 Loss 1.2256 Accuracy 0.4656\n",
      "Epoch 4 Batch 2650 Loss 1.2281 Accuracy 0.4654\n",
      "Epoch 4 Batch 2700 Loss 1.2304 Accuracy 0.4653\n",
      "Epoch 4 Batch 2750 Loss 1.2329 Accuracy 0.4651\n",
      "Epoch 4 Batch 2800 Loss 1.2349 Accuracy 0.4650\n",
      "Epoch 4 Batch 2850 Loss 1.2369 Accuracy 0.4649\n",
      "Epoch 4 Batch 2900 Loss 1.2395 Accuracy 0.4646\n",
      "Epoch 4 Batch 2950 Loss 1.2416 Accuracy 0.4643\n",
      "Epoch 4 Batch 3000 Loss 1.2437 Accuracy 0.4640\n",
      "Epoch 4 Batch 3050 Loss 1.2452 Accuracy 0.4638\n",
      "Epoch 4 Batch 3100 Loss 1.2465 Accuracy 0.4637\n",
      "Epoch 4 Batch 3150 Loss 1.2480 Accuracy 0.4635\n",
      "Epoch 4 Batch 3200 Loss 1.2490 Accuracy 0.4634\n",
      "Epoch 4 Batch 3250 Loss 1.2505 Accuracy 0.4633\n",
      "Time taken for 1 epoch 3266.9046025276184 secs\n",
      "\n",
      "Start or epoch 5\n",
      "Epoch 5 Batch 0 Loss 1.1026 Accuracy 0.4777\n",
      "Epoch 5 Batch 50 Loss 1.3757 Accuracy 0.4549\n",
      "Epoch 5 Batch 100 Loss 1.3496 Accuracy 0.4571\n",
      "Epoch 5 Batch 150 Loss 1.3554 Accuracy 0.4562\n",
      "Epoch 5 Batch 200 Loss 1.3501 Accuracy 0.4561\n",
      "Epoch 5 Batch 250 Loss 1.3448 Accuracy 0.4572\n",
      "Epoch 5 Batch 300 Loss 1.3437 Accuracy 0.4566\n",
      "Epoch 5 Batch 350 Loss 1.3330 Accuracy 0.4570\n",
      "Epoch 5 Batch 400 Loss 1.3199 Accuracy 0.4573\n",
      "Epoch 5 Batch 450 Loss 1.3116 Accuracy 0.4574\n",
      "Epoch 5 Batch 500 Loss 1.3019 Accuracy 0.4585\n",
      "Epoch 5 Batch 550 Loss 1.2955 Accuracy 0.4591\n",
      "Epoch 5 Batch 600 Loss 1.2877 Accuracy 0.4601\n",
      "Epoch 5 Batch 650 Loss 1.2814 Accuracy 0.4602\n",
      "Epoch 5 Batch 700 Loss 1.2762 Accuracy 0.4604\n",
      "Epoch 5 Batch 750 Loss 1.2708 Accuracy 0.4607\n",
      "Epoch 5 Batch 800 Loss 1.2647 Accuracy 0.4614\n",
      "Epoch 5 Batch 850 Loss 1.2604 Accuracy 0.4621\n",
      "Epoch 5 Batch 900 Loss 1.2574 Accuracy 0.4623\n",
      "Epoch 5 Batch 950 Loss 1.2516 Accuracy 0.4627\n",
      "Epoch 5 Batch 1000 Loss 1.2455 Accuracy 0.4632\n",
      "Epoch 5 Batch 1050 Loss 1.2385 Accuracy 0.4632\n",
      "Epoch 5 Batch 1100 Loss 1.2311 Accuracy 0.4634\n",
      "Epoch 5 Batch 1150 Loss 1.2238 Accuracy 0.4637\n",
      "Epoch 5 Batch 1200 Loss 1.2161 Accuracy 0.4647\n",
      "Epoch 5 Batch 1250 Loss 1.2096 Accuracy 0.4653\n",
      "Epoch 5 Batch 1300 Loss 1.2049 Accuracy 0.4659\n",
      "Epoch 5 Batch 1350 Loss 1.1989 Accuracy 0.4665\n",
      "Epoch 5 Batch 1400 Loss 1.1934 Accuracy 0.4669\n",
      "Epoch 5 Batch 1450 Loss 1.1888 Accuracy 0.4675\n",
      "Epoch 5 Batch 1500 Loss 1.1836 Accuracy 0.4684\n",
      "Epoch 5 Batch 1550 Loss 1.1794 Accuracy 0.4691\n",
      "Epoch 5 Batch 1600 Loss 1.1751 Accuracy 0.4698\n",
      "Epoch 5 Batch 1650 Loss 1.1705 Accuracy 0.4705\n",
      "Epoch 5 Batch 1700 Loss 1.1664 Accuracy 0.4710\n",
      "Epoch 5 Batch 1750 Loss 1.1615 Accuracy 0.4717\n",
      "Epoch 5 Batch 1800 Loss 1.1570 Accuracy 0.4724\n",
      "Epoch 5 Batch 1850 Loss 1.1532 Accuracy 0.4730\n",
      "Epoch 5 Batch 1900 Loss 1.1497 Accuracy 0.4737\n",
      "Epoch 5 Batch 1950 Loss 1.1457 Accuracy 0.4745\n",
      "Epoch 5 Batch 2000 Loss 1.1422 Accuracy 0.4752\n",
      "Epoch 5 Batch 2050 Loss 1.1389 Accuracy 0.4759\n",
      "Epoch 5 Batch 2100 Loss 1.1359 Accuracy 0.4766\n",
      "Epoch 5 Batch 2150 Loss 1.1331 Accuracy 0.4772\n",
      "Epoch 5 Batch 2200 Loss 1.1312 Accuracy 0.4777\n",
      "Epoch 5 Batch 2250 Loss 1.1298 Accuracy 0.4782\n",
      "Epoch 5 Batch 2300 Loss 1.1297 Accuracy 0.4786\n",
      "Epoch 5 Batch 2350 Loss 1.1298 Accuracy 0.4787\n",
      "Epoch 5 Batch 2400 Loss 1.1309 Accuracy 0.4788\n",
      "Epoch 5 Batch 2450 Loss 1.1326 Accuracy 0.4787\n",
      "Epoch 5 Batch 2500 Loss 1.1349 Accuracy 0.4785\n",
      "Epoch 5 Batch 2550 Loss 1.1372 Accuracy 0.4784\n",
      "Epoch 5 Batch 2600 Loss 1.1400 Accuracy 0.4783\n",
      "Epoch 5 Batch 2650 Loss 1.1421 Accuracy 0.4781\n",
      "Epoch 5 Batch 2700 Loss 1.1443 Accuracy 0.4779\n",
      "Epoch 5 Batch 2750 Loss 1.1464 Accuracy 0.4778\n",
      "Epoch 5 Batch 2800 Loss 1.1492 Accuracy 0.4776\n",
      "Epoch 5 Batch 2850 Loss 1.1513 Accuracy 0.4773\n",
      "Epoch 5 Batch 2900 Loss 1.1536 Accuracy 0.4771\n",
      "Epoch 5 Batch 2950 Loss 1.1553 Accuracy 0.4768\n",
      "Epoch 5 Batch 3000 Loss 1.1576 Accuracy 0.4766\n",
      "Epoch 5 Batch 3050 Loss 1.1592 Accuracy 0.4764\n",
      "Epoch 5 Batch 3100 Loss 1.1612 Accuracy 0.4761\n",
      "Epoch 5 Batch 3150 Loss 1.1630 Accuracy 0.4759\n",
      "Epoch 5 Batch 3200 Loss 1.1647 Accuracy 0.4757\n",
      "Epoch 5 Batch 3250 Loss 1.1664 Accuracy 0.4755\n",
      "Time taken for 1 epoch 3235.048312664032 secs\n",
      "\n",
      "Start or epoch 6\n",
      "Epoch 6 Batch 0 Loss 1.3717 Accuracy 0.4721\n",
      "Epoch 6 Batch 50 Loss 1.3161 Accuracy 0.4613\n",
      "Epoch 6 Batch 100 Loss 1.2984 Accuracy 0.4620\n",
      "Epoch 6 Batch 150 Loss 1.2862 Accuracy 0.4634\n",
      "Epoch 6 Batch 200 Loss 1.2825 Accuracy 0.4641\n",
      "Epoch 6 Batch 250 Loss 1.2766 Accuracy 0.4653\n",
      "Epoch 6 Batch 300 Loss 1.2669 Accuracy 0.4655\n",
      "Epoch 6 Batch 350 Loss 1.2633 Accuracy 0.4661\n",
      "Epoch 6 Batch 400 Loss 1.2523 Accuracy 0.4672\n",
      "Epoch 6 Batch 450 Loss 1.2439 Accuracy 0.4679\n",
      "Epoch 6 Batch 500 Loss 1.2333 Accuracy 0.4688\n",
      "Epoch 6 Batch 550 Loss 1.2264 Accuracy 0.4696\n",
      "Epoch 6 Batch 600 Loss 1.2190 Accuracy 0.4697\n",
      "Epoch 6 Batch 650 Loss 1.2126 Accuracy 0.4705\n",
      "Epoch 6 Batch 700 Loss 1.2072 Accuracy 0.4706\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6 Batch 750 Loss 1.2025 Accuracy 0.4709\n",
      "Epoch 6 Batch 800 Loss 1.1982 Accuracy 0.4713\n",
      "Epoch 6 Batch 850 Loss 1.1924 Accuracy 0.4717\n",
      "Epoch 6 Batch 900 Loss 1.1881 Accuracy 0.4720\n",
      "Epoch 6 Batch 950 Loss 1.1815 Accuracy 0.4722\n",
      "Epoch 6 Batch 1000 Loss 1.1755 Accuracy 0.4727\n",
      "Epoch 6 Batch 1050 Loss 1.1691 Accuracy 0.4730\n",
      "Epoch 6 Batch 1100 Loss 1.1639 Accuracy 0.4733\n",
      "Epoch 6 Batch 1150 Loss 1.1567 Accuracy 0.4738\n",
      "Epoch 6 Batch 1200 Loss 1.1499 Accuracy 0.4743\n",
      "Epoch 6 Batch 1250 Loss 1.1444 Accuracy 0.4746\n",
      "Epoch 6 Batch 1300 Loss 1.1387 Accuracy 0.4750\n",
      "Epoch 6 Batch 1350 Loss 1.1335 Accuracy 0.4755\n",
      "Epoch 6 Batch 1400 Loss 1.1282 Accuracy 0.4761\n",
      "Epoch 6 Batch 1450 Loss 1.1240 Accuracy 0.4766\n",
      "Epoch 6 Batch 1500 Loss 1.1193 Accuracy 0.4772\n",
      "Epoch 6 Batch 1550 Loss 1.1154 Accuracy 0.4779\n",
      "Epoch 6 Batch 1600 Loss 1.1105 Accuracy 0.4786\n",
      "Epoch 6 Batch 1650 Loss 1.1072 Accuracy 0.4794\n",
      "Epoch 6 Batch 1700 Loss 1.1023 Accuracy 0.4800\n",
      "Epoch 6 Batch 1750 Loss 1.0985 Accuracy 0.4808\n",
      "Epoch 6 Batch 1800 Loss 1.0943 Accuracy 0.4814\n",
      "Epoch 6 Batch 1850 Loss 1.0911 Accuracy 0.4822\n",
      "Epoch 6 Batch 1900 Loss 1.0885 Accuracy 0.4828\n",
      "Epoch 6 Batch 1950 Loss 1.0857 Accuracy 0.4833\n",
      "Epoch 6 Batch 2000 Loss 1.0828 Accuracy 0.4839\n",
      "Epoch 6 Batch 2050 Loss 1.0798 Accuracy 0.4846\n",
      "Epoch 6 Batch 2100 Loss 1.0773 Accuracy 0.4853\n",
      "Epoch 6 Batch 2150 Loss 1.0745 Accuracy 0.4858\n",
      "Epoch 6 Batch 2200 Loss 1.0721 Accuracy 0.4865\n",
      "Epoch 6 Batch 2250 Loss 1.0703 Accuracy 0.4868\n",
      "Epoch 6 Batch 2300 Loss 1.0701 Accuracy 0.4871\n",
      "Epoch 6 Batch 2350 Loss 1.0708 Accuracy 0.4873\n",
      "Epoch 6 Batch 2400 Loss 1.0720 Accuracy 0.4873\n",
      "Epoch 6 Batch 2450 Loss 1.0729 Accuracy 0.4873\n",
      "Epoch 6 Batch 2500 Loss 1.0751 Accuracy 0.4871\n",
      "Epoch 6 Batch 2550 Loss 1.0774 Accuracy 0.4870\n",
      "Epoch 6 Batch 2600 Loss 1.0794 Accuracy 0.4870\n",
      "Epoch 6 Batch 2650 Loss 1.0824 Accuracy 0.4866\n",
      "Epoch 6 Batch 2700 Loss 1.0846 Accuracy 0.4864\n",
      "Epoch 6 Batch 2750 Loss 1.0873 Accuracy 0.4864\n",
      "Epoch 6 Batch 2800 Loss 1.0896 Accuracy 0.4862\n",
      "Epoch 6 Batch 2850 Loss 1.0920 Accuracy 0.4859\n",
      "Epoch 6 Batch 2900 Loss 1.0944 Accuracy 0.4856\n",
      "Epoch 6 Batch 2950 Loss 1.0972 Accuracy 0.4854\n",
      "Epoch 6 Batch 3000 Loss 1.1001 Accuracy 0.4851\n",
      "Epoch 6 Batch 3050 Loss 1.1024 Accuracy 0.4848\n",
      "Epoch 6 Batch 3100 Loss 1.1044 Accuracy 0.4846\n",
      "Epoch 6 Batch 3150 Loss 1.1062 Accuracy 0.4844\n",
      "Epoch 6 Batch 3200 Loss 1.1082 Accuracy 0.4841\n",
      "Epoch 6 Batch 3250 Loss 1.1098 Accuracy 0.4840\n",
      "Time taken for 1 epoch 3239.3037207126617 secs\n",
      "\n",
      "Start or epoch 7\n",
      "Epoch 7 Batch 0 Loss 1.5801 Accuracy 0.4788\n",
      "Epoch 7 Batch 50 Loss 1.2496 Accuracy 0.4719\n",
      "Epoch 7 Batch 100 Loss 1.2435 Accuracy 0.4733\n",
      "Epoch 7 Batch 150 Loss 1.2388 Accuracy 0.4727\n",
      "Epoch 7 Batch 200 Loss 1.2379 Accuracy 0.4734\n",
      "Epoch 7 Batch 250 Loss 1.2292 Accuracy 0.4738\n",
      "Epoch 7 Batch 300 Loss 1.2213 Accuracy 0.4736\n",
      "Epoch 7 Batch 350 Loss 1.2136 Accuracy 0.4746\n",
      "Epoch 7 Batch 400 Loss 1.2061 Accuracy 0.4754\n",
      "Epoch 7 Batch 450 Loss 1.1953 Accuracy 0.4758\n",
      "Epoch 7 Batch 500 Loss 1.1895 Accuracy 0.4762\n",
      "Epoch 7 Batch 550 Loss 1.1815 Accuracy 0.4769\n",
      "Epoch 7 Batch 600 Loss 1.1759 Accuracy 0.4770\n",
      "Epoch 7 Batch 650 Loss 1.1692 Accuracy 0.4771\n",
      "Epoch 7 Batch 700 Loss 1.1640 Accuracy 0.4777\n",
      "Epoch 7 Batch 750 Loss 1.1591 Accuracy 0.4782\n",
      "Epoch 7 Batch 800 Loss 1.1552 Accuracy 0.4786\n",
      "Epoch 7 Batch 850 Loss 1.1497 Accuracy 0.4790\n",
      "Epoch 7 Batch 900 Loss 1.1443 Accuracy 0.4795\n",
      "Epoch 7 Batch 950 Loss 1.1389 Accuracy 0.4796\n",
      "Epoch 7 Batch 1000 Loss 1.1324 Accuracy 0.4796\n",
      "Epoch 7 Batch 1050 Loss 1.1255 Accuracy 0.4799\n",
      "Epoch 7 Batch 1100 Loss 1.1177 Accuracy 0.4803\n",
      "Epoch 7 Batch 1150 Loss 1.1117 Accuracy 0.4809\n",
      "Epoch 7 Batch 1200 Loss 1.1054 Accuracy 0.4813\n",
      "Epoch 7 Batch 1250 Loss 1.0999 Accuracy 0.4814\n",
      "Epoch 7 Batch 1300 Loss 1.0941 Accuracy 0.4818\n",
      "Epoch 7 Batch 1350 Loss 1.0892 Accuracy 0.4824\n",
      "Epoch 7 Batch 1400 Loss 1.0836 Accuracy 0.4830\n",
      "Epoch 7 Batch 1450 Loss 1.0793 Accuracy 0.4836\n",
      "Epoch 7 Batch 1500 Loss 1.0744 Accuracy 0.4842\n",
      "Epoch 7 Batch 1550 Loss 1.0701 Accuracy 0.4849\n",
      "Epoch 7 Batch 1600 Loss 1.0659 Accuracy 0.4857\n",
      "Epoch 7 Batch 1650 Loss 1.0617 Accuracy 0.4863\n",
      "Epoch 7 Batch 1700 Loss 1.0578 Accuracy 0.4870\n",
      "Epoch 7 Batch 1750 Loss 1.0539 Accuracy 0.4877\n",
      "Epoch 7 Batch 1800 Loss 1.0496 Accuracy 0.4883\n",
      "Epoch 7 Batch 1850 Loss 1.0472 Accuracy 0.4890\n",
      "Epoch 7 Batch 1900 Loss 1.0441 Accuracy 0.4896\n",
      "Epoch 7 Batch 1950 Loss 1.0406 Accuracy 0.4902\n",
      "Epoch 7 Batch 2000 Loss 1.0378 Accuracy 0.4909\n",
      "Epoch 7 Batch 2050 Loss 1.0351 Accuracy 0.4916\n",
      "Epoch 7 Batch 2100 Loss 1.0328 Accuracy 0.4922\n",
      "Epoch 7 Batch 2150 Loss 1.0305 Accuracy 0.4928\n",
      "Epoch 7 Batch 2200 Loss 1.0280 Accuracy 0.4933\n",
      "Epoch 7 Batch 2250 Loss 1.0269 Accuracy 0.4937\n",
      "Epoch 7 Batch 2300 Loss 1.0256 Accuracy 0.4938\n",
      "Epoch 7 Batch 2350 Loss 1.0258 Accuracy 0.4940\n",
      "Epoch 7 Batch 2400 Loss 1.0273 Accuracy 0.4940\n",
      "Epoch 7 Batch 2450 Loss 1.0287 Accuracy 0.4939\n",
      "Epoch 7 Batch 2500 Loss 1.0310 Accuracy 0.4939\n",
      "Epoch 7 Batch 2550 Loss 1.0338 Accuracy 0.4937\n",
      "Epoch 7 Batch 2600 Loss 1.0359 Accuracy 0.4936\n",
      "Epoch 7 Batch 2650 Loss 1.0385 Accuracy 0.4934\n",
      "Epoch 7 Batch 2700 Loss 1.0412 Accuracy 0.4932\n",
      "Epoch 7 Batch 2750 Loss 1.0441 Accuracy 0.4929\n",
      "Epoch 7 Batch 2800 Loss 1.0470 Accuracy 0.4927\n",
      "Epoch 7 Batch 2850 Loss 1.0495 Accuracy 0.4925\n",
      "Epoch 7 Batch 2900 Loss 1.0522 Accuracy 0.4921\n",
      "Epoch 7 Batch 2950 Loss 1.0545 Accuracy 0.4918\n",
      "Epoch 7 Batch 3000 Loss 1.0572 Accuracy 0.4915\n",
      "Epoch 7 Batch 3050 Loss 1.0599 Accuracy 0.4912\n",
      "Epoch 7 Batch 3100 Loss 1.0618 Accuracy 0.4910\n",
      "Epoch 7 Batch 3150 Loss 1.0636 Accuracy 0.4907\n",
      "Epoch 7 Batch 3200 Loss 1.0655 Accuracy 0.4906\n",
      "Epoch 7 Batch 3250 Loss 1.0669 Accuracy 0.4904\n",
      "Time taken for 1 epoch 3188.073582649231 secs\n",
      "\n",
      "Start or epoch 8\n",
      "Epoch 8 Batch 0 Loss 1.2731 Accuracy 0.4844\n",
      "Epoch 8 Batch 50 Loss 1.2331 Accuracy 0.4790\n",
      "Epoch 8 Batch 100 Loss 1.2185 Accuracy 0.4787\n",
      "Epoch 8 Batch 150 Loss 1.2062 Accuracy 0.4784\n",
      "Epoch 8 Batch 200 Loss 1.1984 Accuracy 0.4789\n",
      "Epoch 8 Batch 250 Loss 1.1850 Accuracy 0.4802\n",
      "Epoch 8 Batch 300 Loss 1.1807 Accuracy 0.4802\n",
      "Epoch 8 Batch 350 Loss 1.1799 Accuracy 0.4802\n",
      "Epoch 8 Batch 400 Loss 1.1702 Accuracy 0.4801\n",
      "Epoch 8 Batch 450 Loss 1.1612 Accuracy 0.4811\n",
      "Epoch 8 Batch 500 Loss 1.1505 Accuracy 0.4811\n",
      "Epoch 8 Batch 550 Loss 1.1430 Accuracy 0.4818\n",
      "Epoch 8 Batch 600 Loss 1.1376 Accuracy 0.4826\n",
      "Epoch 8 Batch 650 Loss 1.1316 Accuracy 0.4827\n",
      "Epoch 8 Batch 700 Loss 1.1256 Accuracy 0.4830\n",
      "Epoch 8 Batch 750 Loss 1.1207 Accuracy 0.4837\n",
      "Epoch 8 Batch 800 Loss 1.1177 Accuracy 0.4844\n",
      "Epoch 8 Batch 850 Loss 1.1131 Accuracy 0.4848\n",
      "Epoch 8 Batch 900 Loss 1.1088 Accuracy 0.4851\n",
      "Epoch 8 Batch 950 Loss 1.1022 Accuracy 0.4850\n",
      "Epoch 8 Batch 1000 Loss 1.0963 Accuracy 0.4855\n",
      "Epoch 8 Batch 1050 Loss 1.0904 Accuracy 0.4858\n",
      "Epoch 8 Batch 1100 Loss 1.0836 Accuracy 0.4861\n",
      "Epoch 8 Batch 1150 Loss 1.0768 Accuracy 0.4863\n",
      "Epoch 8 Batch 1200 Loss 1.0709 Accuracy 0.4867\n",
      "Epoch 8 Batch 1250 Loss 1.0638 Accuracy 0.4870\n",
      "Epoch 8 Batch 1300 Loss 1.0580 Accuracy 0.4875\n",
      "Epoch 8 Batch 1350 Loss 1.0522 Accuracy 0.4882\n",
      "Epoch 8 Batch 1400 Loss 1.0466 Accuracy 0.4888\n",
      "Epoch 8 Batch 1450 Loss 1.0413 Accuracy 0.4894\n",
      "Epoch 8 Batch 1500 Loss 1.0373 Accuracy 0.4899\n",
      "Epoch 8 Batch 1550 Loss 1.0328 Accuracy 0.4907\n",
      "Epoch 8 Batch 1600 Loss 1.0282 Accuracy 0.4913\n",
      "Epoch 8 Batch 1650 Loss 1.0245 Accuracy 0.4917\n",
      "Epoch 8 Batch 1700 Loss 1.0209 Accuracy 0.4922\n",
      "Epoch 8 Batch 1750 Loss 1.0179 Accuracy 0.4929\n",
      "Epoch 8 Batch 1800 Loss 1.0137 Accuracy 0.4936\n",
      "Epoch 8 Batch 1850 Loss 1.0111 Accuracy 0.4943\n",
      "Epoch 8 Batch 1900 Loss 1.0082 Accuracy 0.4949\n",
      "Epoch 8 Batch 1950 Loss 1.0052 Accuracy 0.4955\n",
      "Epoch 8 Batch 2000 Loss 1.0022 Accuracy 0.4961\n",
      "Epoch 8 Batch 2050 Loss 1.0002 Accuracy 0.4968\n",
      "Epoch 8 Batch 2100 Loss 0.9975 Accuracy 0.4977\n",
      "Epoch 8 Batch 2150 Loss 0.9949 Accuracy 0.4982\n",
      "Epoch 8 Batch 2200 Loss 0.9924 Accuracy 0.4986\n",
      "Epoch 8 Batch 2250 Loss 0.9918 Accuracy 0.4989\n",
      "Epoch 8 Batch 2300 Loss 0.9916 Accuracy 0.4991\n",
      "Epoch 8 Batch 2350 Loss 0.9918 Accuracy 0.4992\n",
      "Epoch 8 Batch 2400 Loss 0.9926 Accuracy 0.4994\n",
      "Epoch 8 Batch 2450 Loss 0.9944 Accuracy 0.4993\n",
      "Epoch 8 Batch 2500 Loss 0.9964 Accuracy 0.4992\n",
      "Epoch 8 Batch 2550 Loss 0.9991 Accuracy 0.4991\n",
      "Epoch 8 Batch 2600 Loss 1.0017 Accuracy 0.4989\n",
      "Epoch 8 Batch 2650 Loss 1.0043 Accuracy 0.4987\n",
      "Epoch 8 Batch 2700 Loss 1.0065 Accuracy 0.4985\n",
      "Epoch 8 Batch 2750 Loss 1.0092 Accuracy 0.4983\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8 Batch 2800 Loss 1.0121 Accuracy 0.4979\n",
      "Epoch 8 Batch 2850 Loss 1.0149 Accuracy 0.4977\n",
      "Epoch 8 Batch 2900 Loss 1.0173 Accuracy 0.4974\n",
      "Epoch 8 Batch 2950 Loss 1.0196 Accuracy 0.4970\n",
      "Epoch 8 Batch 3000 Loss 1.0220 Accuracy 0.4967\n",
      "Epoch 8 Batch 3050 Loss 1.0242 Accuracy 0.4964\n",
      "Epoch 8 Batch 3100 Loss 1.0264 Accuracy 0.4963\n",
      "Epoch 8 Batch 3150 Loss 1.0289 Accuracy 0.4959\n",
      "Epoch 8 Batch 3200 Loss 1.0309 Accuracy 0.4957\n",
      "Epoch 8 Batch 3250 Loss 1.0332 Accuracy 0.4955\n",
      "Time taken for 1 epoch 3226.426320552826 secs\n",
      "\n",
      "Start or epoch 9\n",
      "Epoch 9 Batch 0 Loss 1.1805 Accuracy 0.4900\n",
      "Epoch 9 Batch 50 Loss 1.2201 Accuracy 0.4770\n",
      "Epoch 9 Batch 100 Loss 1.1712 Accuracy 0.4806\n",
      "Epoch 9 Batch 150 Loss 1.1619 Accuracy 0.4807\n",
      "Epoch 9 Batch 200 Loss 1.1608 Accuracy 0.4818\n",
      "Epoch 9 Batch 250 Loss 1.1503 Accuracy 0.4837\n",
      "Epoch 9 Batch 300 Loss 1.1450 Accuracy 0.4841\n",
      "Epoch 9 Batch 350 Loss 1.1404 Accuracy 0.4841\n",
      "Epoch 9 Batch 400 Loss 1.1312 Accuracy 0.4848\n",
      "Epoch 9 Batch 450 Loss 1.1254 Accuracy 0.4855\n",
      "Epoch 9 Batch 500 Loss 1.1177 Accuracy 0.4862\n",
      "Epoch 9 Batch 550 Loss 1.1104 Accuracy 0.4865\n",
      "Epoch 9 Batch 600 Loss 1.1041 Accuracy 0.4864\n",
      "Epoch 9 Batch 650 Loss 1.0995 Accuracy 0.4865\n",
      "Epoch 9 Batch 700 Loss 1.0925 Accuracy 0.4873\n",
      "Epoch 9 Batch 750 Loss 1.0851 Accuracy 0.4875\n",
      "Epoch 9 Batch 800 Loss 1.0818 Accuracy 0.4879\n",
      "Epoch 9 Batch 850 Loss 1.0774 Accuracy 0.4881\n",
      "Epoch 9 Batch 900 Loss 1.0743 Accuracy 0.4883\n",
      "Epoch 9 Batch 950 Loss 1.0694 Accuracy 0.4887\n",
      "Epoch 9 Batch 1000 Loss 1.0645 Accuracy 0.4891\n",
      "Epoch 9 Batch 1050 Loss 1.0594 Accuracy 0.4895\n",
      "Epoch 9 Batch 1100 Loss 1.0523 Accuracy 0.4897\n",
      "Epoch 9 Batch 1150 Loss 1.0455 Accuracy 0.4901\n",
      "Epoch 9 Batch 1200 Loss 1.0397 Accuracy 0.4905\n",
      "Epoch 9 Batch 1250 Loss 1.0331 Accuracy 0.4908\n",
      "Epoch 9 Batch 1300 Loss 1.0281 Accuracy 0.4914\n",
      "Epoch 9 Batch 1350 Loss 1.0234 Accuracy 0.4920\n",
      "Epoch 9 Batch 1400 Loss 1.0178 Accuracy 0.4925\n",
      "Epoch 9 Batch 1450 Loss 1.0131 Accuracy 0.4935\n",
      "Epoch 9 Batch 1500 Loss 1.0086 Accuracy 0.4941\n",
      "Epoch 9 Batch 1550 Loss 1.0040 Accuracy 0.4946\n",
      "Epoch 9 Batch 1600 Loss 0.9998 Accuracy 0.4953\n",
      "Epoch 9 Batch 1650 Loss 0.9962 Accuracy 0.4959\n",
      "Epoch 9 Batch 1700 Loss 0.9930 Accuracy 0.4966\n",
      "Epoch 9 Batch 1750 Loss 0.9894 Accuracy 0.4970\n",
      "Epoch 9 Batch 1800 Loss 0.9860 Accuracy 0.4978\n",
      "Epoch 9 Batch 1850 Loss 0.9830 Accuracy 0.4984\n",
      "Epoch 9 Batch 1900 Loss 0.9800 Accuracy 0.4991\n",
      "Epoch 9 Batch 1950 Loss 0.9774 Accuracy 0.4995\n",
      "Epoch 9 Batch 2000 Loss 0.9746 Accuracy 0.5002\n",
      "Epoch 9 Batch 2050 Loss 0.9725 Accuracy 0.5007\n",
      "Epoch 9 Batch 2100 Loss 0.9702 Accuracy 0.5014\n",
      "Epoch 9 Batch 2150 Loss 0.9684 Accuracy 0.5020\n",
      "Epoch 9 Batch 2200 Loss 0.9662 Accuracy 0.5025\n",
      "Epoch 9 Batch 2250 Loss 0.9647 Accuracy 0.5030\n",
      "Epoch 9 Batch 2300 Loss 0.9638 Accuracy 0.5032\n",
      "Epoch 9 Batch 2350 Loss 0.9646 Accuracy 0.5033\n",
      "Epoch 9 Batch 2400 Loss 0.9661 Accuracy 0.5032\n",
      "Epoch 9 Batch 2450 Loss 0.9672 Accuracy 0.5032\n",
      "Epoch 9 Batch 2500 Loss 0.9696 Accuracy 0.5031\n",
      "Epoch 9 Batch 2550 Loss 0.9723 Accuracy 0.5029\n",
      "Epoch 9 Batch 2600 Loss 0.9749 Accuracy 0.5028\n",
      "Epoch 9 Batch 2650 Loss 0.9771 Accuracy 0.5026\n",
      "Epoch 9 Batch 2700 Loss 0.9796 Accuracy 0.5024\n",
      "Epoch 9 Batch 2750 Loss 0.9823 Accuracy 0.5022\n",
      "Epoch 9 Batch 2800 Loss 0.9851 Accuracy 0.5020\n",
      "Epoch 9 Batch 2850 Loss 0.9874 Accuracy 0.5018\n",
      "Epoch 9 Batch 2900 Loss 0.9905 Accuracy 0.5014\n",
      "Epoch 9 Batch 2950 Loss 0.9928 Accuracy 0.5010\n",
      "Epoch 9 Batch 3000 Loss 0.9953 Accuracy 0.5007\n",
      "Epoch 9 Batch 3050 Loss 0.9973 Accuracy 0.5004\n",
      "Epoch 9 Batch 3100 Loss 0.9997 Accuracy 0.5001\n",
      "Epoch 9 Batch 3150 Loss 1.0020 Accuracy 0.4999\n",
      "Epoch 9 Batch 3200 Loss 1.0046 Accuracy 0.4997\n",
      "Epoch 9 Batch 3250 Loss 1.0065 Accuracy 0.4995\n",
      "Time taken for 1 epoch 3200.5910544395447 secs\n",
      "\n",
      "Start or epoch 10\n",
      "Epoch 10 Batch 0 Loss 1.1593 Accuracy 0.4107\n",
      "Epoch 10 Batch 50 Loss 1.1769 Accuracy 0.4847\n",
      "Epoch 10 Batch 100 Loss 1.1507 Accuracy 0.4854\n",
      "Epoch 10 Batch 150 Loss 1.1450 Accuracy 0.4862\n",
      "Epoch 10 Batch 200 Loss 1.1360 Accuracy 0.4866\n",
      "Epoch 10 Batch 250 Loss 1.1326 Accuracy 0.4876\n",
      "Epoch 10 Batch 300 Loss 1.1261 Accuracy 0.4880\n",
      "Epoch 10 Batch 350 Loss 1.1144 Accuracy 0.4884\n",
      "Epoch 10 Batch 400 Loss 1.1070 Accuracy 0.4883\n",
      "Epoch 10 Batch 450 Loss 1.1024 Accuracy 0.4891\n",
      "Epoch 10 Batch 500 Loss 1.0931 Accuracy 0.4903\n",
      "Epoch 10 Batch 550 Loss 1.0874 Accuracy 0.4904\n",
      "Epoch 10 Batch 600 Loss 1.0814 Accuracy 0.4905\n",
      "Epoch 10 Batch 650 Loss 1.0770 Accuracy 0.4905\n",
      "Epoch 10 Batch 700 Loss 1.0708 Accuracy 0.4910\n",
      "Epoch 10 Batch 750 Loss 1.0660 Accuracy 0.4913\n",
      "Epoch 10 Batch 800 Loss 1.0617 Accuracy 0.4918\n",
      "Epoch 10 Batch 850 Loss 1.0566 Accuracy 0.4922\n",
      "Epoch 10 Batch 900 Loss 1.0507 Accuracy 0.4928\n",
      "Epoch 10 Batch 950 Loss 1.0469 Accuracy 0.4929\n",
      "Epoch 10 Batch 1000 Loss 1.0411 Accuracy 0.4930\n",
      "Epoch 10 Batch 1050 Loss 1.0362 Accuracy 0.4932\n",
      "Epoch 10 Batch 1100 Loss 1.0292 Accuracy 0.4934\n",
      "Epoch 10 Batch 1150 Loss 1.0229 Accuracy 0.4942\n",
      "Epoch 10 Batch 1200 Loss 1.0172 Accuracy 0.4945\n",
      "Epoch 10 Batch 1250 Loss 1.0109 Accuracy 0.4951\n",
      "Epoch 10 Batch 1300 Loss 1.0046 Accuracy 0.4956\n",
      "Epoch 10 Batch 1350 Loss 0.9998 Accuracy 0.4962\n",
      "Epoch 10 Batch 1400 Loss 0.9945 Accuracy 0.4967\n",
      "Epoch 10 Batch 1450 Loss 0.9904 Accuracy 0.4970\n",
      "Epoch 10 Batch 1500 Loss 0.9858 Accuracy 0.4978\n",
      "Epoch 10 Batch 1550 Loss 0.9818 Accuracy 0.4984\n",
      "Epoch 10 Batch 1600 Loss 0.9783 Accuracy 0.4991\n",
      "Epoch 10 Batch 1650 Loss 0.9742 Accuracy 0.4997\n",
      "Epoch 10 Batch 1700 Loss 0.9701 Accuracy 0.5001\n",
      "Epoch 10 Batch 1750 Loss 0.9663 Accuracy 0.5008\n",
      "Epoch 10 Batch 1800 Loss 0.9630 Accuracy 0.5014\n",
      "Epoch 10 Batch 1850 Loss 0.9600 Accuracy 0.5020\n",
      "Epoch 10 Batch 1900 Loss 0.9572 Accuracy 0.5025\n",
      "Epoch 10 Batch 1950 Loss 0.9547 Accuracy 0.5030\n",
      "Epoch 10 Batch 2000 Loss 0.9519 Accuracy 0.5036\n",
      "Epoch 10 Batch 2050 Loss 0.9489 Accuracy 0.5042\n",
      "Epoch 10 Batch 2100 Loss 0.9467 Accuracy 0.5050\n",
      "Epoch 10 Batch 2150 Loss 0.9447 Accuracy 0.5055\n",
      "Epoch 10 Batch 2200 Loss 0.9422 Accuracy 0.5061\n",
      "Epoch 10 Batch 2250 Loss 0.9407 Accuracy 0.5066\n",
      "Epoch 10 Batch 2300 Loss 0.9405 Accuracy 0.5070\n",
      "Epoch 10 Batch 2350 Loss 0.9408 Accuracy 0.5072\n",
      "Epoch 10 Batch 2400 Loss 0.9420 Accuracy 0.5073\n",
      "Epoch 10 Batch 2450 Loss 0.9435 Accuracy 0.5071\n",
      "Epoch 10 Batch 2500 Loss 0.9461 Accuracy 0.5071\n",
      "Epoch 10 Batch 2550 Loss 0.9488 Accuracy 0.5068\n",
      "Epoch 10 Batch 2600 Loss 0.9513 Accuracy 0.5066\n",
      "Epoch 10 Batch 2650 Loss 0.9541 Accuracy 0.5064\n",
      "Epoch 10 Batch 2700 Loss 0.9570 Accuracy 0.5061\n",
      "Epoch 10 Batch 2750 Loss 0.9598 Accuracy 0.5058\n",
      "Epoch 10 Batch 2800 Loss 0.9628 Accuracy 0.5055\n",
      "Epoch 10 Batch 2850 Loss 0.9657 Accuracy 0.5052\n",
      "Epoch 10 Batch 2900 Loss 0.9685 Accuracy 0.5049\n",
      "Epoch 10 Batch 2950 Loss 0.9709 Accuracy 0.5046\n",
      "Epoch 10 Batch 3000 Loss 0.9736 Accuracy 0.5044\n",
      "Epoch 10 Batch 3050 Loss 0.9757 Accuracy 0.5042\n",
      "Epoch 10 Batch 3100 Loss 0.9780 Accuracy 0.5039\n",
      "Epoch 10 Batch 3150 Loss 0.9801 Accuracy 0.5037\n",
      "Epoch 10 Batch 3200 Loss 0.9822 Accuracy 0.5033\n",
      "Epoch 10 Batch 3250 Loss 0.9842 Accuracy 0.5031\n",
      "Time taken for 1 epoch 3206.140262365341 secs\n",
      "\n"
     ]
    }
   ],
   "source": [
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    print('Start or epoch {}'.format(epoch + 1))\n",
    "    start = time.time()\n",
    "\n",
    "    train_loss.reset_states()\n",
    "    train_accuracy.reset_states()\n",
    "\n",
    "    for (batch, (enc_inputs, targets)) in enumerate(dataset):\n",
    "        dec_inputs = targets[:, :-1]\n",
    "        dec_outputs_real = targets[:, 1:]\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = transformer(enc_inputs, dec_inputs, True)\n",
    "            loss = loss_function(dec_outputs_real, predictions)\n",
    "    \n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "\n",
    "        train_loss(loss)\n",
    "        train_accuracy(dec_outputs_real, predictions)\n",
    "   \n",
    "        if batch % 50 == 0:\n",
    "            print('Epoch {} Batch {} Loss {:.4f} Accuracy {:.4f}'.format(epoch+1, batch, train_loss.result(), train_accuracy.result()))\n",
    "            \n",
    "    print('Time taken for 1 epoch {} secs\\n'.format(time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49c2702",
   "metadata": {
    "id": "nmzyRwDrRGdq"
   },
   "source": [
    "# Avaliação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "779da1ed",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "_0pqI1RTpzwk",
    "outputId": "50b0fc38-c599-4831-a72e-ad620ec7ead7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8188, 55, 17, 2201, 4093, 8189]"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = 'you are smart'\n",
    "text = [vocab_size_en - 2] + tokenizer_en.encode(text) + [vocab_size_en - 1]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "17003c02",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "dvGS3gUJqHI2",
    "outputId": "641cf2ce-472f-454d-ef3c-c50a25829463"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 6])"
      ]
     },
     "execution_count": 304,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = tf.expand_dims(text, axis=0)\n",
    "text.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "ac7eca9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "VAS70m-xq9M5",
    "outputId": "45244d41-a4a1-40b9-a613-c2ad756f0f0d"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([1, 1])"
      ]
     },
     "execution_count": 305,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = tf.expand_dims([vocab_size_pt - 2], axis = 0)\n",
    "output.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "917e71f1",
   "metadata": {
    "id": "XDtQcdvHqcVi"
   },
   "outputs": [],
   "source": [
    "def evaluate(inp_sentence):\n",
    "  inp_sentence = [vocab_size_en - 2] + tokenizer_en.encode(inp_sentence) + [vocab_size_en - 1]\n",
    "  enc_input = tf.expand_dims(inp_sentence, axis=0)\n",
    "\n",
    "  output = tf.expand_dims([vocab_size_pt - 2], axis = 0)\n",
    "\n",
    "  # i am -> am happy\n",
    "\n",
    "  for _ in range(max_length):\n",
    "    # (1, seq_length, vocab_size)\n",
    "    predictions = transformer(enc_input, output, False)\n",
    "    prediction = predictions[:, -1:, :]\n",
    "\n",
    "    predicted_id = tf.cast(tf.argmax(prediction, axis=-1), tf.int32)\n",
    "\n",
    "    if predicted_id == vocab_size_pt - 1:\n",
    "      return tf.squeeze(output, axis=0)\n",
    "\n",
    "    output = tf.concat([output, predicted_id], axis=1)\n",
    "  \n",
    "  return tf.squeeze(output, axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "aa309f82",
   "metadata": {
    "id": "RyAABcHRtcms"
   },
   "outputs": [],
   "source": [
    "def translate(sentence):\n",
    "  output = evaluate(sentence).numpy()\n",
    "\n",
    "  predicted_sentence = tokenizer_pt.decode([i for i in output if i < vocab_size_pt - 2])\n",
    "  \n",
    "  print('Input: {}'.format(sentence))\n",
    "  print('Predicted translation: {}'.format(predicted_sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "a4f78f8d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "id": "vfLt3wZmt3wa",
    "outputId": "ddcdd5bd-e717-4727-a353-2b24c5606ff6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: this is a really powerful tool\n",
      "Predicted translation: Também isto é um instrumento verdadeiramente poderoso.\n"
     ]
    }
   ],
   "source": [
    "translate(\"this is a really powerful tool\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
